{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OUBRwgEu3yu1"
   },
   "source": [
    "# Práctica 2: Procesamiento del Lenguaje Natural\n",
    "\n",
    "__Fecha de entrega: 8 de mayo de 2023__\n",
    "\n",
    "El objetivo de esta práctica es aplicar los conceptos teóricos vistos en clase en el módulo de PLN. La práctica consta de 2 notebooks que se entregarán simultáneamente en la tarea de entrega habilitada en el Campus  Virtual.\n",
    "\n",
    "Lo más importante en esta práctica no es el código Python, sino el análisis de los datos y modelos que construyas y las explicaciones razonadas de cada una de las decisiones que tomes. __No se valorarán trozos de código o gráficas sin ningún tipo de contexto o explicación__.\n",
    "\n",
    "Finalmente, recuerda establecer el parámetro `random_state` en todas las funciones que tomen decisiones aleatorias para que los resultados sean reproducibles (los resultados no varíen entre ejecuciones)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "V3YxCTUW3yu9"
   },
   "outputs": [],
   "source": [
    "RANDOM_STATE = 333"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pn_YQLVL3yvA"
   },
   "source": [
    "# Apartado 1: Análisis de sentimientos con word embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "de-i8w0s3yvC"
   },
   "source": [
    "__Nombres de los estudiantes: Enrique Queipo de Llano Burgos y Alejandro Paz Olalla__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yeVD_g2D3yvC"
   },
   "source": [
    "## 1) Carga del conjunto de datos\n",
    "\n",
    "El fichero `IMBD_Dataset.csv` contiene opiniones de películas clasificadas en 2 categorías diferentes (positiva/negativa).\n",
    "\n",
    "Este set de datos se creó utilizando el \"IMDB Dataset of 50K Movie Reviews\", el cual contiene 50,000 reseñas de películas con un sentimiento positivo o negativo adjunto a ellas.\n",
    "\n",
    "Muestra un ejemplo de cada clase.\n",
    "\n",
    "Haz un estudio del conjunto de datos. ¿qué palabras aparecen más veces?, ¿tendría sentido normalizar de alguna manera el corpus?\n",
    "\n",
    "Crea una partición de los datos dejando el 80% para entrenamiento y el 20% restante para test usando la función `train_test_split` de sklearn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1487,
     "status": "ok",
     "timestamp": 1681627835928,
     "user": {
      "displayName": "ALBERTO DIAZ ESTEBAN",
      "userId": "09370147929418307454"
     },
     "user_tz": -120
    },
    "id": "5YyPy4BDfzGQ",
    "outputId": "d3637bed-fd90-4b68-9372-ed77292ae301",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# acceso a google drive\n",
    "\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "#!pip uninstall keras --yes\n",
    "#!pip install --upgrade pip\n",
    "#!pip uninstall tensorflow --yes\n",
    "#!pip install tensorflow\n",
    "#!pip install numpy==1.22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "0csu2B8N3yvE"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "import re\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 920,
     "status": "ok",
     "timestamp": 1681627836845,
     "user": {
      "displayName": "ALBERTO DIAZ ESTEBAN",
      "userId": "09370147929418307454"
     },
     "user_tz": -120
    },
    "id": "kmcEfPgYf1Fk",
    "outputId": "e7aed290-7b25-478a-8bed-3872fd756c12"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imbd_file = 'IMDB_Dataset.csv'\n",
    "\n",
    "df=pd.read_csv(imbd_file)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     50000 non-null  object\n",
      " 1   sentiment  50000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 781.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive review:  [['The minutiae of what\\'s involved in carrying out a robbery is what makes this one of the best of all heist movies. Then there\\'s the robbery itself, a wordless, thirty minute nail-biter that has never been surpassed, followed by what is probably the cinema\\'s most pronounced example of dishonor among thieves as things begin to spectacularly unravel, and we have what is unquestionably the greatest of all heist movies.<br /><br />This was a tough and unsentimental film when it first appeared in 1955 and it is just as tough and unsentimental today. (It displays some of the edgy brutality of Dassin\\'s earlier \"Brute Force\"). There isn\\'t a flabby moment or duff performance in the entire film and Dassin captures the milieu of seedy clubs and Parisian back streets like no-one else and the final drive through Paris by a dying man is one of the most iconic closing sequences of any movie. A classic.'\n",
      "  'positive']]\n",
      "Negative review:  [[\"Crude, some times crass - to me that's the summation of Madhur Bhandarkar's latest work - Page 3. He has no point of view - just shallow, funny digs at stereotypes. What is the movie about?? Is it about reporting a clan of people (so called Page 3 types) who are so busy socializing and progressing their profiles in life - that they have no time for anything else. And you are either in it or out of it. Is it that there is no press at all to report everyday incidents. Madhur Bhandarkar forgets that there is a main newspaper and Page 3 is just a supplement; perhaps an entertainer for checking out who's who and what's what. Don't mix the two. And then there is power play - that would happen in every walk of life. So what have you told at the end of it all - nothing - just a few crude jokes strung together in an otherwise direction less movie.\"\n",
      "  'negative']]\n"
     ]
    }
   ],
   "source": [
    "positive = df[df['sentiment'] == 'positive'].sample(random_state=RANDOM_STATE)\n",
    "positive_index = positive.index.values\n",
    "negative = df[df['sentiment'] == 'negative'].sample(random_state=RANDOM_STATE)\n",
    "negative_index = negative.index.values\n",
    "print('Positive review: ',positive.values)\n",
    "print('Negative review: ',negative.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive    25000\n",
      "negative    25000\n",
      "Name: sentiment, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='sentiment', ylabel='count'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(df['sentiment'].value_counts())\n",
    "sns.countplot(x = \"sentiment\", data = df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review  [29064] [\"I'm surprised to read all the positive comments on this movie. Even my 4 and 6 year old were bored. The chipmunks are cute...but the storyline is overly obvious. Not recommended for young ones with the least sophisticated tastes.<br /><br />We did hear a few laughs from the audience while we were in attendance; but I wondered why.<br /><br />I don't admit to ever being a 'chipmunks' fan, but I expected to be entertained. It's not even an 'escapist' movie as far as I can tell. Simply a heavy handed view of 'success too young spoils'. We've seen more than enough of that with Britney Spears, et al, haven't we?<br /><br />Don't bother.\"]\n"
     ]
    }
   ],
   "source": [
    "X = df['review']\n",
    "y = df['sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE,stratify=y)\n",
    "train_sample = X_train.sample(random_state=RANDOM_STATE)\n",
    "train_sample_index = train_sample.index.values\n",
    "print('Review ', train_sample_index, train_sample.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0.5, 1.0, 'Distribution of the train set')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x=y_train).set(title='Distribution of the train set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0.5, 1.0, 'Distribution of the test set')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x=y_test).set(title='Distribution of the test set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, codificamos en binario los ``sentiment``. ES NECESARIO ESTO PARA PODER ENTRENAR EL MODELO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  One of the other reviewers has mentioned that ...          1\n",
       "1  A wonderful little production. <br /><br />The...          1\n",
       "2  I thought this was a wonderful way to spend ti...          1\n",
       "3  Basically there's a family where a little boy ...          0\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...          1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos que las distribuciones en los conjuntos de entrenamiento y prueba son prácticamente iguales y no tenemos class imbalance.\n",
    "\n",
    "A continuación, queremos ver qué palabras son las que más se repiten. Para ello, vectorizaremos el corpus y sumaremos cada fila para calcular cuántas veces aparece cada palabra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<50000x101895 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 6826529 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Primero vectorizamos las reviews\n",
    "cv = CountVectorizer()\n",
    "cv_matrix = cv.fit_transform(df['review'])\n",
    "cv_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 62917,  64110, 101096,  33226,  13654,  99740,  34443,  60214,\n",
       "         6166,  98149,  90137,  90455,  44763,  46902,  12041,  46765,\n",
       "        91217,  63757,   4541,  90160])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nº de veces que aparece cada palabra == suma de su correspondiente columna\n",
    "# Sacamos las sumas\n",
    "totales = np.squeeze(np.asarray(np.sum(cv_matrix, axis = 0)))\n",
    "# Ordenamos y nos quedamos con los 20 mejores índices\n",
    "top20 = np.argsort(totales)[-20:]\n",
    "top20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['not', 'on', 'you', 'film', 'but', 'with', 'for', 'movie', 'as',\n",
       "       'was', 'that', 'this', 'in', 'it', 'br', 'is', 'to', 'of', 'and',\n",
       "       'the'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Enseñamos las 20 palabras más repetidas\n",
    "cv.get_feature_names_out()[top20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que las palabras que más aparecen (como tabmién es de esperar) son preposiciones, pronombres, etc que no aportan mucho a la clasificación del documento, por tanto será importante normalizarlo para eliminar estas stopwords. Este es el caso de palabras como ``this``, ``that``, ``and``, ``you``... o incluso de la notación html para el salto de línea ``br``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "XCSCi12Ut7UkGvGJLt8fS9",
     "report_properties": {
      "rowId": "eVGmQHoecOeCIqwoY4qBbO"
     },
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/quiquequeipodellano/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "wpt = nltk.WordPunctTokenizer()\n",
    "nltk.download('stopwords')\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def normalize_document(doc):\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    # tokenize document\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    # re-create document from filtered tokens\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "normalize_corpus = np.vectorize(normalize_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "KHE4fFH811y18qcAhCZ7VB",
     "report_properties": {
      "rowId": "puBTTEPjB3CgjHepQ5spUC"
     },
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['one reviewers mentioned watching oz episode youll hooked right exactly happened mebr br first thing struck oz brutality unflinching scenes violence set right word go trust show faint hearted timid show pulls punches regards drugs sex violence hardcore classic use wordbr br called oz nickname given oswald maximum security state penitentary focuses mainly emerald city experimental section prison cells glass fronts face inwards privacy high agenda em city home manyaryans muslims gangstas latinos christians italians irish moreso scuffles death stares dodgy dealings shady agreements never far awaybr br would say main appeal show due fact goes shows wouldnt dare forget pretty pictures painted mainstream audiences forget charm forget romanceoz doesnt mess around first episode ever saw struck nasty surreal couldnt say ready watched developed taste oz got accustomed high levels graphic violence violence injustice crooked guards wholl sold nickel inmates wholl kill order get away well mannered middle class inmates turned prison bitches due lack street skills prison experience watching oz may become comfortable uncomfortable viewingthats get touch darker side',\n",
       "       'wonderful little production br br filming technique unassuming oldtimebbc fashion gives comforting sometimes discomforting sense realism entire piece br br actors extremely well chosen michael sheen got polari voices pat truly see seamless editing guided references williams diary entries well worth watching terrificly written performed piece masterful production one great masters comedy life br br realism really comes home little things fantasy guard rather use traditional dream techniques remains solid disappears plays knowledge senses particularly scenes concerning orton halliwell sets particularly flat halliwells murals decorating every surface terribly well done',\n",
       "       'thought wonderful way spend time hot summer weekend sitting air conditioned theater watching lighthearted comedy plot simplistic dialogue witty characters likable even well bread suspected serial killer may disappointed realize match point risk addiction thought proof woody allen still fully control style many us grown lovebr br id laughed one woodys comedies years dare say decade ive never impressed scarlet johanson managed tone sexy image jumped right average spirited young womanbr br may crown jewel career wittier devil wears prada interesting superman great comedy go see friends',\n",
       "       ...,\n",
       "       'catholic taught parochial elementary schools nuns taught jesuit priests high school college still practicing catholic would considered good catholic churchs eyes dont believe certain things act certain ways church tells tobr br back movieits bad two people killed nun supposed satire embodiment female religious figurehead comedy satire done well acting diane keaton never saw play different movies may goodbr br first thought gun might fake first shooting plan female lead four former students attempt demonstrate sister marys emotional intellectual bigotry faith turns bullets real story tragedythe tragedy loss life besides two former studentsthe lives aborted babies life students mom tragedy dogmatic authority love people tragedy organized religion replacing true faith god wrong todays islam yesterdays judaism christianity',\n",
       "       'im going disagree previous comment side maltin one second rate excessively vicious western creaks groans trying put across central theme wild west tamed kicked aside steady march time would like tradition butch cassidy sundance kid lacks films poignancy charm andrew mclaglens direction limp final minutes real botch incomprehensible strategy part heroes charlton heston chris mitchum someone give holler explain set hillside fire something callous whole treatment rape scene womans reaction afterwards certainly ring true coburn plenty nasty half breed escaped convict revenge fellow escapees underdeveloped theyre like bowling pins knocked one one story lurches forward michael parks gives one typically shifty lethargic mumbling performances case appropriate modern style sheriff symbolizes complacency technological progress bring',\n",
       "       'one expects star trek movies high art fans expect movie good best episodes unfortunately movie muddled implausible plot left cringing far worst nine far movies even chance watch well known characters interact another movie cant save movie including goofy scenes kirk spock mccoy yosemitebr br would say movie worth rental hardly worth watching however true fan needs see movies renting movie way youll see even cable channels avoid movie'],\n",
       "      dtype='<U9505')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_corpus = normalize_corpus(df['review'])\n",
    "norm_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<50000x175583 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 4987630 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Primero vectorizamos las reviews\n",
    "cv = CountVectorizer()\n",
    "cv_matrix = cv.fit_transform(norm_corpus)\n",
    "cv_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 55513, 114935,  10843,   4525,  64488,  61478, 169350, 101915,\n",
       "       148067, 136112, 125769, 156651, 172826,  49868,  63233,  88361,\n",
       "       109560,  54626, 101168,  18449])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nº de veces que aparece cada palabra == suma de su correspondiente columna\n",
    "# Sacamos las sumas\n",
    "totales = np.squeeze(np.asarray(np.sum(cv_matrix, axis = 0)))\n",
    "# Ordenamos y nos quedamos con los 20 mejores índices\n",
    "top20 = np.argsort(totales)[-20:]\n",
    "top20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['first', 'people', 'bad', 'also', 'great', 'get', 'well', 'much',\n",
       "       'story', 'see', 'really', 'time', 'would', 'even', 'good', 'like',\n",
       "       'one', 'film', 'movie', 'br'], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Enseñamos las 20 palabras más repetidas\n",
    "cv.get_feature_names_out()[top20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez normalizado el documento, vemos que el top20 de palabras más repetidas ha pasado a tener adejtivos y sustantivos que sí proyectan cierto significado, aunque no nos hemos librado del ``br``. En concreto nos interesarán palabras como ``bad`` o ``great`` ya que incluyen el significado positivo/negativo que andamos buscando para la clasificación de los textos del problema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r4rXv3xX3yvG"
   },
   "source": [
    "## 2) Estudio del efecto de distintas configuraciones de word embeddings para resolver la tarea\n",
    "\n",
    "Usa distintas configuraciones de word embeddings y discute los resultados obtenidos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primero entrenaremos nuestro propio word_embedding (Modelo 1) y después usaremos modelos pre-entrenados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MODELO 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para comenzar con los word embeddings, debo calcular el parámetro ``num_words`` que le pasaremos a `Tokenizer`. Un enfoque común es elegir un valor de ``num_words`` que sea un poco mayor que el número de palabras únicas en tus datos, y luego ajustarlo según sea necesario. Para obtener una estimación aproximada del número de palabras únicas en tus datos, puedes contar el número de palabras totales en tus reviews y luego dividirlo por la longitud promedio de las reviews. Calculamos primero la longitud media de las reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "6WLFEg9B6Uu9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número total de palabras:  6072576\n",
      "Longitud media en palabras:  121.45152\n",
      "Max words:  49999.0\n"
     ]
    }
   ],
   "source": [
    "# Suma todas las filas de la matriz de conteo de palabras\n",
    "word_counts = cv_matrix.sum(axis=1)\n",
    "\n",
    "# Calcula la longitud media en términos de cantidad de palabras\n",
    "mean_words = word_counts.mean()\n",
    "\n",
    "# Suma todos los elementos de la matriz de conteo de palabras\n",
    "total_words = cv_matrix.sum()\n",
    "\n",
    "print(\"Número total de palabras: \", total_words)\n",
    "print(\"Longitud media en palabras: \", mean_words)\n",
    "print(\"Max words: \", total_words // mean_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debemos calcular también el parámetro ``maxlen`` de pad_sequences. Tomaremos la longitud máxima de una review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longitud máxima:  1426\n"
     ]
    }
   ],
   "source": [
    "max_length = np.max(np.squeeze(np.asarray(np.sum(cv_matrix,axis=1))))\n",
    "print(\"Longitud máxima: \", max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OBSERVACIÓN**\n",
    "Los parámetros que pretendíamos usar eran:\n",
    "- embedding_dim = 250. En el tutorial se usa un valor de 50 para 1000 datos, en este caso son 50,000 reviews, por lo que hemos pensado en multiplicar en 5 el número de dimensiones.\n",
    "- max_words: En el tutorial se usa un valor incial (un tanto arbitrario) de 1500. Nosotros pretendíamos usar el número de palabras únicas. Para estimar esto podemos calcular el número total de palabras y dividirlo entre la longitud media de cada review. La suma de toda la matriz de CountVectorizer nos da el número total de palabras, que dividimos entre la longitud media. En total nos da un valor de 49,999\n",
    "- max_comment_length: En el tutorial se usa un valor (un tanto arbitrario) de 20. En nuestro caso, las reviews son bastante más largas que los datos del tutorial. Por tanto, hemos decidido tomar para este parámetro la longitud de la review más larga.\n",
    "\n",
    "SIN EMBARGO, tomar estos valores implicaría que ejecutar la celda que entrena con 20 epochs tardaría demasiado (ver más abajo la celda del modelo1 con Sequential, Embedding, Flatten y Dense), aunque sí que arroja muy buenos valores para la precisión. Igualmente, preferiremos reducir estos valores a algunos más pequeños arbitrariamente en pos de que el entrenamiento se vuelva más tratable. Esto, en parte, también se debe a que el conjunto de datos es más grande, por lo que tenemos que tener cuidado dando valores grandes a los parámetros ya que también implicarán más cálculos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 175617 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "max_words = total_words // mean_words\n",
    "max_comment_length = max_length\n",
    "\n",
    "max_words = 1000\n",
    "max_comment_length = 20\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(norm_corpus)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(norm_corpus)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "max_words = len(word_index)\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=max_comment_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training texts: 40000\n",
      "Test texts: 10000\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, df['sentiment'], test_size=0.20, random_state=RANDOM_STATE,stratify = df.sentiment)\n",
    "\n",
    "print(\"Training texts:\", len(y_train))\n",
    "print(\"Test texts:\", len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CON LOS VALORES: embedding_dim = 40, max_words = 1000 y max_comment_length = 20 tarda ~~ 120s por epoca --> TOTAL aproximado de 40 MINUTOS. AUMENTAR ESTOS VALORES AUMENTA MUCHO EL TIEMPO DE EJECUCIÓN DE LA CELDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, 20, 40)            7024680   \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 800)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 801       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,025,481\n",
      "Trainable params: 7,025,481\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "1250/1250 [==============================] - 138s 107ms/step - loss: 0.5002 - accuracy: 0.7577 - val_loss: 0.4563 - val_accuracy: 0.7806\n",
      "Epoch 2/20\n",
      "1250/1250 [==============================] - 126s 101ms/step - loss: 0.4165 - accuracy: 0.8064 - val_loss: 0.4563 - val_accuracy: 0.7843\n",
      "Epoch 3/20\n",
      "1250/1250 [==============================] - 119s 95ms/step - loss: 0.3837 - accuracy: 0.8285 - val_loss: 0.4707 - val_accuracy: 0.7785\n",
      "Epoch 4/20\n",
      "1250/1250 [==============================] - 120s 96ms/step - loss: 0.3478 - accuracy: 0.8482 - val_loss: 0.4937 - val_accuracy: 0.7715\n",
      "Epoch 5/20\n",
      "1250/1250 [==============================] - 124s 99ms/step - loss: 0.3158 - accuracy: 0.8673 - val_loss: 0.5244 - val_accuracy: 0.7616\n",
      "Epoch 6/20\n",
      "1250/1250 [==============================] - 132s 106ms/step - loss: 0.2887 - accuracy: 0.8818 - val_loss: 0.5575 - val_accuracy: 0.7552\n",
      "Epoch 7/20\n",
      "1250/1250 [==============================] - 133s 106ms/step - loss: 0.2652 - accuracy: 0.8942 - val_loss: 0.5963 - val_accuracy: 0.7462\n",
      "Epoch 8/20\n",
      "1250/1250 [==============================] - 130s 104ms/step - loss: 0.2454 - accuracy: 0.9036 - val_loss: 0.6364 - val_accuracy: 0.7460\n",
      "Epoch 9/20\n",
      "1250/1250 [==============================] - 127s 102ms/step - loss: 0.2286 - accuracy: 0.9135 - val_loss: 0.6793 - val_accuracy: 0.7411\n",
      "Epoch 10/20\n",
      "1250/1250 [==============================] - 120s 96ms/step - loss: 0.2135 - accuracy: 0.9187 - val_loss: 0.7266 - val_accuracy: 0.7378\n",
      "Epoch 11/20\n",
      "1250/1250 [==============================] - 122s 97ms/step - loss: 0.2005 - accuracy: 0.9269 - val_loss: 0.7750 - val_accuracy: 0.7316\n",
      "Epoch 12/20\n",
      "1250/1250 [==============================] - 123s 98ms/step - loss: 0.1882 - accuracy: 0.9316 - val_loss: 0.8284 - val_accuracy: 0.7298\n",
      "Epoch 13/20\n",
      "1250/1250 [==============================] - 123s 99ms/step - loss: 0.1774 - accuracy: 0.9379 - val_loss: 0.8792 - val_accuracy: 0.7266\n",
      "Epoch 14/20\n",
      "1250/1250 [==============================] - 125s 100ms/step - loss: 0.1669 - accuracy: 0.9426 - val_loss: 0.9325 - val_accuracy: 0.7262\n",
      "Epoch 15/20\n",
      "1250/1250 [==============================] - 117s 93ms/step - loss: 0.1574 - accuracy: 0.9472 - val_loss: 0.9923 - val_accuracy: 0.7207\n",
      "Epoch 16/20\n",
      "1250/1250 [==============================] - 115s 92ms/step - loss: 0.1487 - accuracy: 0.9504 - val_loss: 1.0459 - val_accuracy: 0.7211\n",
      "Epoch 17/20\n",
      "1250/1250 [==============================] - 3878s 3s/step - loss: 0.1403 - accuracy: 0.9549 - val_loss: 1.1054 - val_accuracy: 0.7161\n",
      "Epoch 18/20\n",
      "1250/1250 [==============================] - 1788s 1s/step - loss: 0.1324 - accuracy: 0.9570 - val_loss: 1.1677 - val_accuracy: 0.7168\n",
      "Epoch 19/20\n",
      "1250/1250 [==============================] - 124s 99ms/step - loss: 0.1250 - accuracy: 0.9604 - val_loss: 1.2325 - val_accuracy: 0.7153\n",
      "Epoch 20/20\n",
      "1250/1250 [==============================] - 102s 81ms/step - loss: 0.1178 - accuracy: 0.9647 - val_loss: 1.2979 - val_accuracy: 0.7108\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 1.2979 - accuracy: 0.7108\n",
      "Accuracy: 71.08%\n"
     ]
    }
   ],
   "source": [
    "# Fijamos el tamaño de los embedding a 100 dimensiones\n",
    "\n",
    "embedding_dim = 40\n",
    "\n",
    "# MODELO 1. SIN EMBEDDINGS PRE-ENTRENADOS \n",
    "\n",
    "model1 = Sequential()\n",
    "# We specify the maximum input length to our Embedding layer\n",
    "# so we can later flatten the embedded inputs\n",
    "\n",
    "\n",
    "model1.add(Embedding(max_words, embedding_dim, input_length=max_comment_length))\n",
    "# After the Embedding layer, our activations have shape `(max_words, max_comment_length, embedding_dim)`.\n",
    "\n",
    "# We flatten the 3D tensor of embeddings into a 2D tensor of shape `(max_words, max_comment_length * embedding_dim)`\n",
    "\n",
    "model1.add(Flatten())\n",
    "\n",
    "# We add the classifier on top\n",
    "model1.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model1.summary()\n",
    "\n",
    "history = model1.fit(X_train, y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(X_test, y_test))\n",
    "\n",
    "score1 = model1.evaluate(X_test, y_test)\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (score1[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos una precisión del 71% en test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MODELOS 2 y 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03QL2jZv5HXr"
   },
   "source": [
    "### Pre-process the embeddings\n",
    "\n",
    "\n",
    "Parseado del fichero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3789,
     "status": "ok",
     "timestamp": 1682186303379,
     "user": {
      "displayName": "ALBERTO DIAZ ESTEBAN",
      "userId": "09370147929418307454"
     },
     "user_tz": -120
    },
    "id": "fEZv4hve5HXr",
    "outputId": "7684d5de-e8cd-4748-f9ea-ee5eb9a796f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "glove_dir = ''\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.50d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vjPFiOfQ5HXr"
   },
   "source": [
    "\n",
    "Construimos una embedding matrix para poder cargar en nuestra capa de `Embedding`. Deber ser una matriz de dimensiones `(max_words, embedding_dim)`, donde cada entrada `i` contiene el vector `embedding_dim`-dimensional para la palabra de índice `i` en nuestra referencia palabra-índice. El índice `0` no representa ningún vector ni palabra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1682186303379,
     "user": {
      "displayName": "ALBERTO DIAZ ESTEBAN",
      "userId": "09370147929418307454"
     },
     "user_tz": -120
    },
    "id": "m5Zs0sIU5HXs"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 50\n",
    "\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if i < max_words:\n",
    "        if embedding_vector is not None:\n",
    "            # Words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zqvm9DNK5HXs"
   },
   "source": [
    "### Definimos el modelo\n",
    "\n",
    "Igual que antes construimos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1682186303380,
     "user": {
      "displayName": "ALBERTO DIAZ ESTEBAN",
      "userId": "09370147929418307454"
     },
     "user_tz": -120
    },
    "id": "QnP6jLBR5HXs",
    "outputId": "07971e98-a075-4b9e-a203-00a7e9b15181"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_5 (Embedding)     (None, 20, 50)            8780850   \n",
      "                                                                 \n",
      " flatten_5 (Flatten)         (None, 1000)              0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 1001      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,781,851\n",
      "Trainable params: 8,781,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# MODELO 2. EMBEDDINGS PRE-ENTRENADOS CONGELADOS\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Embedding(max_words, embedding_dim, input_length=max_comment_length))\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aq9tCKoe5HXs"
   },
   "source": [
    "### Load the GloVe embeddings in the model\n",
    "\n",
    "\n",
    "Cargamos la matriz que hemos construido a nuestra capa `Embedding`, que es la primera de nuestro modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1682186303380,
     "user": {
      "displayName": "ALBERTO DIAZ ESTEBAN",
      "userId": "09370147929418307454"
     },
     "user_tz": -120
    },
    "id": "x38tZ6Fn5HXs"
   },
   "outputs": [],
   "source": [
    "model2.layers[0].set_weights([embedding_matrix])\n",
    "model2.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6KHAcFgG5HXt"
   },
   "source": [
    "Además, hemos \"congelado\" la capa de `Embedding` (poniendo `trainable` a `False`), entonces una parte del modelo es pre-entrenada (como nuestra capa `Embedding`), y otras partes son inicializadas aleatoriamente (como el clasificador), las partes pre-entrenadas no deben ser actualizadas durante el entrenamiento para evitar que se olviden de lo que ya saben."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HcWGxxEU5HXt"
   },
   "source": [
    "### Train and evaluate\n",
    "\n",
    "Aquí entrenamos y evaluamos el modelo construido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3272,
     "status": "ok",
     "timestamp": 1682186306645,
     "user": {
      "displayName": "ALBERTO DIAZ ESTEBAN",
      "userId": "09370147929418307454"
     },
     "user_tz": -120
    },
    "id": "EMK_DYeI5HXt",
    "outputId": "73c4b493-eb4d-44ec-9f57-e9bfc24cb742"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 0.6132 - accuracy: 0.6601 - val_loss: 0.5840 - val_accuracy: 0.6934\n",
      "Epoch 2/20\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.5725 - accuracy: 0.7004 - val_loss: 0.5797 - val_accuracy: 0.6936\n",
      "Epoch 3/20\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 0.5685 - accuracy: 0.7058 - val_loss: 0.5774 - val_accuracy: 0.6933\n",
      "Epoch 4/20\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.5668 - accuracy: 0.7046 - val_loss: 0.5845 - val_accuracy: 0.6913\n",
      "Epoch 5/20\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 0.5664 - accuracy: 0.7067 - val_loss: 0.5797 - val_accuracy: 0.6896\n",
      "Epoch 6/20\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 0.5661 - accuracy: 0.7089 - val_loss: 0.5784 - val_accuracy: 0.6917\n",
      "Epoch 7/20\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.5661 - accuracy: 0.7065 - val_loss: 0.5785 - val_accuracy: 0.6918\n",
      "Epoch 8/20\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.5657 - accuracy: 0.7054 - val_loss: 0.5902 - val_accuracy: 0.6877\n",
      "Epoch 9/20\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.5655 - accuracy: 0.7076 - val_loss: 0.5799 - val_accuracy: 0.6901\n",
      "Epoch 10/20\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 0.5670 - accuracy: 0.7068 - val_loss: 0.5782 - val_accuracy: 0.6941\n",
      "Epoch 11/20\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 0.5659 - accuracy: 0.7084 - val_loss: 0.5779 - val_accuracy: 0.6971\n",
      "Epoch 12/20\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 0.5653 - accuracy: 0.7078 - val_loss: 0.5815 - val_accuracy: 0.6956\n",
      "Epoch 13/20\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 0.5660 - accuracy: 0.7063 - val_loss: 0.5813 - val_accuracy: 0.6933\n",
      "Epoch 14/20\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 0.5655 - accuracy: 0.7081 - val_loss: 0.5873 - val_accuracy: 0.6908\n",
      "Epoch 15/20\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 0.5652 - accuracy: 0.7065 - val_loss: 0.5832 - val_accuracy: 0.6934\n",
      "Epoch 16/20\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 0.5657 - accuracy: 0.7065 - val_loss: 0.5792 - val_accuracy: 0.6928\n",
      "Epoch 17/20\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 0.5660 - accuracy: 0.7071 - val_loss: 0.5861 - val_accuracy: 0.6922\n",
      "Epoch 18/20\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 0.5663 - accuracy: 0.7066 - val_loss: 0.5852 - val_accuracy: 0.6934\n",
      "Epoch 19/20\n",
      "1250/1250 [==============================] - 3s 2ms/step - loss: 0.5654 - accuracy: 0.7080 - val_loss: 0.5819 - val_accuracy: 0.6894\n",
      "Epoch 20/20\n",
      "1250/1250 [==============================] - 3s 3ms/step - loss: 0.5654 - accuracy: 0.7074 - val_loss: 0.5950 - val_accuracy: 0.6880\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.5950 - accuracy: 0.6880\n"
     ]
    }
   ],
   "source": [
    "model2.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model2.fit(X_train, y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(X_test, y_test))\n",
    "\n",
    "score2 = model2.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03mPNZ2j5HXu"
   },
   "source": [
    "Obtenemos en torno al 70% de precisión con el modelo pre-entrenado. Es un mejor resultado ya que es computacionalmente mucho menos costoso\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vIUu9Xed9qXE"
   },
   "source": [
    "También podemos probar a entrenar el mismo modelo sin congelar la capa `Embedding`. En ese caso, estaríamos adaptando también nuestro word-embedding aprendiendo una tarea de clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3332,
     "status": "ok",
     "timestamp": 1682186309967,
     "user": {
      "displayName": "ALBERTO DIAZ ESTEBAN",
      "userId": "09370147929418307454"
     },
     "user_tz": -120
    },
    "id": "QtegIcSE5HXu",
    "outputId": "d410cbe9-6569-49be-b51e-03fdfcee06ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_6 (Embedding)     (None, 20, 50)            8780850   \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (None, 1000)              0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 1001      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,781,851\n",
      "Trainable params: 8,781,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "1250/1250 [==============================] - 149s 119ms/step - loss: 0.5496 - accuracy: 0.7099 - val_loss: 0.4822 - val_accuracy: 0.7695\n",
      "Epoch 2/20\n",
      "1250/1250 [==============================] - 136s 109ms/step - loss: 0.4395 - accuracy: 0.7935 - val_loss: 0.4622 - val_accuracy: 0.7798\n",
      "Epoch 3/20\n",
      "1250/1250 [==============================] - 139s 111ms/step - loss: 0.4140 - accuracy: 0.8098 - val_loss: 0.4621 - val_accuracy: 0.7822\n",
      "Epoch 4/20\n",
      "1250/1250 [==============================] - 138s 110ms/step - loss: 0.3957 - accuracy: 0.8194 - val_loss: 0.4692 - val_accuracy: 0.7791\n",
      "Epoch 5/20\n",
      "1250/1250 [==============================] - 125s 100ms/step - loss: 0.3766 - accuracy: 0.8312 - val_loss: 0.4809 - val_accuracy: 0.7748\n",
      "Epoch 6/20\n",
      "1250/1250 [==============================] - 121s 97ms/step - loss: 0.3579 - accuracy: 0.8404 - val_loss: 0.4979 - val_accuracy: 0.7718\n",
      "Epoch 7/20\n",
      "1250/1250 [==============================] - 118s 95ms/step - loss: 0.3397 - accuracy: 0.8497 - val_loss: 0.5123 - val_accuracy: 0.7680\n",
      "Epoch 8/20\n",
      "1250/1250 [==============================] - 118s 95ms/step - loss: 0.3220 - accuracy: 0.8607 - val_loss: 0.5341 - val_accuracy: 0.7590\n",
      "Epoch 9/20\n",
      "1250/1250 [==============================] - 119s 95ms/step - loss: 0.3060 - accuracy: 0.8700 - val_loss: 0.5547 - val_accuracy: 0.7595\n",
      "Epoch 10/20\n",
      "1250/1250 [==============================] - 1927s 2s/step - loss: 0.2907 - accuracy: 0.8777 - val_loss: 0.5798 - val_accuracy: 0.7551\n",
      "Epoch 11/20\n",
      "1250/1250 [==============================] - 15291s 12s/step - loss: 0.2768 - accuracy: 0.8850 - val_loss: 0.6038 - val_accuracy: 0.7502\n",
      "Epoch 12/20\n",
      "1250/1250 [==============================] - 11950s 10s/step - loss: 0.2643 - accuracy: 0.8924 - val_loss: 0.6332 - val_accuracy: 0.7445\n",
      "Epoch 13/20\n",
      "1250/1250 [==============================] - 160s 128ms/step - loss: 0.2506 - accuracy: 0.8986 - val_loss: 0.6606 - val_accuracy: 0.7451\n",
      "Epoch 14/20\n",
      "1250/1250 [==============================] - 161s 129ms/step - loss: 0.2388 - accuracy: 0.9060 - val_loss: 0.6951 - val_accuracy: 0.7391\n",
      "Epoch 15/20\n",
      "1250/1250 [==============================] - 1206s 965ms/step - loss: 0.2284 - accuracy: 0.9098 - val_loss: 0.7254 - val_accuracy: 0.7364\n",
      "Epoch 16/20\n",
      "1250/1250 [==============================] - 1200s 960ms/step - loss: 0.2174 - accuracy: 0.9158 - val_loss: 0.7586 - val_accuracy: 0.7371\n",
      "Epoch 17/20\n",
      "1250/1250 [==============================] - 1858s 1s/step - loss: 0.2082 - accuracy: 0.9195 - val_loss: 0.7939 - val_accuracy: 0.7334\n",
      "Epoch 18/20\n",
      "1250/1250 [==============================] - 152s 121ms/step - loss: 0.1984 - accuracy: 0.9262 - val_loss: 0.8322 - val_accuracy: 0.7288\n",
      "Epoch 19/20\n",
      "1250/1250 [==============================] - 143s 114ms/step - loss: 0.1899 - accuracy: 0.9304 - val_loss: 0.8680 - val_accuracy: 0.7288\n",
      "Epoch 20/20\n",
      "1250/1250 [==============================] - 140s 112ms/step - loss: 0.1805 - accuracy: 0.9359 - val_loss: 0.9059 - val_accuracy: 0.7284\n",
      "313/313 [==============================] - 0s 2ms/step - loss: 0.9059 - accuracy: 0.7284\n"
     ]
    }
   ],
   "source": [
    "# MODELO3. EMBEDDINGS PREENTRENADOS SIN CONGELAR\n",
    "\n",
    "model3 = Sequential()\n",
    "model3.add(Embedding(max_words, embedding_dim, input_length=max_comment_length))\n",
    "model3.add(Flatten())\n",
    "model3.add(Dense(1, activation='sigmoid'))\n",
    "model3.summary()\n",
    "\n",
    "model3.layers[0].set_weights([embedding_matrix])\n",
    "model3.layers[0].trainable = True\n",
    "\n",
    "model3.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history = model3.fit(X_train, y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(X_test, y_test))\n",
    "\n",
    "score3 = model3.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jvppc0cR5HXu"
   },
   "source": [
    "Obtenemos un 73% de precisión con el modelo pre-entrenado con configurado. En este caso, es la mejor configuración para nuestro problema de clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ycQc3CyjFNxI"
   },
   "source": [
    "# Resumen de resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XmM1ftJe3yvK"
   },
   "source": [
    "## 3) Análisis final\n",
    "\n",
    "Analiza con detalle el mejor clasificador. Busca un ejemplo mal clasificado de cada clase, justifica el error ¿se te ocurre alguna forma de solucionarlo?\n",
    "\n",
    "Compara los resultados obtenidos con y sin word embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 200,
     "status": "ok",
     "timestamp": 1682187099603,
     "user": {
      "displayName": "ALBERTO DIAZ ESTEBAN",
      "userId": "09370147929418307454"
     },
     "user_tz": -120
    },
    "id": "PG05YT6hssVI",
    "outputId": "96c17eec-0a13-413b-dab0-7ad8787a11a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sin word embeddings pre-entrenados\n",
      "Accuracy: 71.08%\n",
      "Con word embeddings pre-entrenados congelados\n",
      "Accuracy: 68.80%\n",
      "Con word embeddings pre-entrenados sin congelar\n",
      "Accuracy: 72.84%\n"
     ]
    }
   ],
   "source": [
    "print(\"Sin word embeddings pre-entrenados\")\n",
    "print(\"Accuracy: %.2f%%\" % (score1[1]*100))\n",
    "print(\"Con word embeddings pre-entrenados congelados\")\n",
    "print(\"Accuracy: %.2f%%\" % (score2[1]*100))\n",
    "print(\"Con word embeddings pre-entrenados sin congelar\")\n",
    "print(\"Accuracy: %.2f%%\" % (score3[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 1ms/step\n",
      "[1. 0. 1. ... 1. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "predictions = model3.predict(X_test)\n",
    "for i in range(len(predictions)):\n",
    "    predictions[i] = 1 if predictions[i] >= 0.5 else 0\n",
    "predictions = np.squeeze(predictions)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo mal clasificado para clase 'positive':  ['All right, let\\'s be realistic about this. Nobody goes into a movie produced by WWE Films (whose owner has challenged God to a wrestling match), directed by a former porn director (the man gave the world the Between the Cheeks trilogy), starring a wrestler named Kane, and expects a little slice of art on a golden platter. If you do then you probably need to find something other than watching movies to occupy your time.<br /><br />So what exactly are we to expect from a movie like this? Well, here\\'s what I was looking forward to:<br /><br />1) Bad acting. 2) A fairly non-existent, clichéd storyline. 3) Kane walking around with a scrunched, sour face that indicates his nostrils just found the potato salad he misplaced a month ago. 4) Tons and tons of gore. <br /><br />Well, if you\\'re hungry for some \"so bad it\\'s funny\" entertainment then this might satisfy your appetite because it delivers on all counts.<br /><br />Obviously, movies like this are best seen for free, but if you do choose to sacrifice box office bucks then have some fun and make a game out of it. The filmmakers are nice enough to introduce us to each of the annoying delinquents by flashing their names and legal offenses on the screen. This makes it easier for you to write down which ones you want to see killed and in what order. You and your friends can see whose predictions are most accurate.<br /><br />I also suggest that you and your pals write down every single moment of stupidity and inanity that you can find. Tally them up at the end and see who comes up with the most. I think my grand total was 107; can you beat that? I personally want to know how after 35 years and a fire does this abandoned hotel still have electricity, running water, and a working elevator?<br /><br />I know, I know, the filmmakers are assuming that if you pay to see this then you obviously don\\'t put much thought into what you spend your money on and therefore likely won\\'t put much thought into how silly the movie is, but that doesn\\'t mean we can\\'t point it out and laugh at it.<br /><br />I also like how the city wants to turn this huge hotel (which would be condemned and recommended for demolition by any sensible inspector) into a homeless shelter and they think the best way to get it cleaned up is to give eight punks a few mops and brooms. Uh-huh.<br /><br />I think you pretty much know what to expect, but I feel the need to provide you with a couple of warnings. First, if you hate crowd interaction no matter the movie then you might want to stay away. The people in the audience acted like they were at an actual wrestling show. Shouts of \"Kill him, Kane!\" and \"I hope you die first!\" and \"Chokeslam!\" echoed through the theater, showcasing what I hope is NOT the best of what America has to offer. I usually don\\'t appreciate such audience interaction, but for a cheesefest like this I thought the commentary added to the entertainment value. However, I can see how others could be annoyed by it.<br /><br />Second, and this shouldn\\'t even warrant explanation, the film doesn\\'t shy away from the gore. If watching a big ugly dude rip eyeballs right out of their sockets doesn\\'t scream \"fun night at the movies!\" for ya then you know good and well to save your dough.<br /><br />I must say that I was a little surprised by the extreme lack of dialogue on Kane\\'s part. I wasn\\'t expecting him to put on an acting clinic, but I was hoping he\\'d have some cute little catchphrase like \"Say goodnight\" (his character\\'s last name is \"Goodnight\") right before he killed a victim. Instead he uttered four words in the entire film - \"Nooooooo!\" and \"I see it.\" But hey, he delivered them flawlessly!<br /><br />If I were a bad guy in a movie then my catchphrase would be something like \"Place your BETTS!\" or \"All BETTS are off!\" <br /><br />It\\'d rule and you know it. We need a new genre term for bad horror films like See No Evil that induce so much unintentional laughter that you almost have to label them comedic. Feel free to send me your suggestions. For now we\\'ll just call \\'em HOR-larious!'\n",
      " 0]\n",
      "Ejemplo mal clasificado para clase 'negative':  [\"If all movies had to be destroyed and only one could be spared, Death in Venice would have to be it. It is a monument in movie history. Much criticized for being slow, boring and too obvious in stating it's point (an old man discovers beauty in a young boy and is tragically destroyed, first mentally, then physically), we should appreciate this movie for what it is. 'Morte a Venezia' was shot over 30 years ago, and it portrays a period even further back, at the beginning of the twentieth century.<br /><br />Life was slow then, compared to now. People were supposed to behave in a certain way, hiding their true emotions even from themselves. Director Visconti and Dirk Bogarde, the leading actor, admirably succeed in showing how the aging composer Von Aschenbach discovers his romantic interest in a young boy. For a man like Von Aschenbach, in his time, this must have been a shock too powerful to come to terms with. We see his inner struggle, mostly on the face of Bogarde, against the beautiful backdrop of Venice and accompanied by the most wonderful music, composed by Gustav Mahler.<br /><br />This movie is slow, there is no denying it. No special effects, car chases or fights to keep the audience pinned to their seats. No perverted sex scenes either; the interaction between man and boy is limited to stolen glances from afar and the occasional smile.<br /><br />So, basically, nothing much happens in this movie? Not if you want your senses to be hit like a base drum. If you want them to be played like the strings of a violin in a romantic concerto, this is the movie to do it.\"\n",
      " 1]\n"
     ]
    }
   ],
   "source": [
    "# Encontrar el índice de los ejemplos que fueron mal clasificados\n",
    "wrong_positives = np.where((predictions == 1) & (y_test == 0))[0]\n",
    "wrong_negatives = np.where((predictions == 0) & (y_test == 1))[0]\n",
    "\n",
    "wp_review = df.iloc[y_test.index[wrong_positives[0]]]\n",
    "wn_review = df.iloc[y_test.index[wrong_negatives[0]]]\n",
    "#print(y_test.iloc[wrong_positives[0]])\n",
    "\n",
    "# Mostrar un ejemplo mal clasificado para cada clase\n",
    "print(\"Ejemplo mal clasificado para clase 'positive': \", wp_review.values)\n",
    "print(\"Ejemplo mal clasificado para clase 'negative': \", wn_review.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qU0UFbSm9NQm"
   },
   "source": [
    "Es posible que las reviews estén mal clasificadas dado que, por las pocas dimensiones con las que contamos, el modelo solo puede tener en cuenta algunas palabras y no el texto entero, que por lo general son largos. Una forma de solucionar esto sería retocar los parámetros de los modelos, pero como hemos dicho antes, se hace bastante costoso en tiempo para algunos modelos."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
